# Real-time serving with embedded model

**Real-time serving with embedded model** is about distributed event-at-a-time processing with millisecond latency and high throughput.

**What to optimize**: latency and throughput

**End user**: usually no direct interactions with a model

**Validation**: offline and online via A/B testing

---
## Advanced workshop: Real-time serving with embedded model
This workshop is WIP

It will cover a real-life use case of embedding a machine learning model into streaming app and its troubleshooting.

---
## Extras

To learn more about real-time serving with embedded ML models:
- [Machine Learning and Real-Time Analytics in Apache Kafka Applications](https://www.confluent.io/blog/machine-learning-real-time-analytics-models-in-kafka-applications/)
- [Kafka Streams machine learning examples](https://github.com/confluentinc/kafka-streams-machine-learning-examples)
- [Streaming Machine Learning at Scale from 100000 IoT Devices with HiveMQ, Apache Kafka and TensorFLow](https://github.com/kaiwaehner/hivemq-mqtt-tensorflow-kafka-realtime-iot-machine-learning-training-inference)
- [Streaming ML Model Deployment](https://github.com/schmidtbri/streaming-ml-model-deployment)
